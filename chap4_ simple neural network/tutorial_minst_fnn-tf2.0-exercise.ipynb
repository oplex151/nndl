{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def mnist_dataset(batch_size=64):\n",
    "    # 定义数据预处理步骤：转换为Tensor以及标准化\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # 将图像转换为Tensor，并且数值范围归一化到[0, 1]\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),  # 使用MNIST数据集的均值和标准差进行标准化\n",
    "    ])\n",
    "\n",
    "    # 下载训练数据集并进行预处理\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 下载测试数据集并进行预处理\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myModel, self).__init__()\n",
    "        ####################\n",
    "        '''声明模型对应的参数'''\n",
    "        ####################        \n",
    "        self.mul_h1 = nn.Linear(28*28, 100)\n",
    "        self.mul_h2 = nn.Linear(100, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####################\n",
    "        '''实现模型函数体，返回未归一化的logits'''\n",
    "        ####################\n",
    "        # 输入x应为PyTorch张量，无需手动添加偏置项，因为Linear层会自动处理\n",
    "        x = x.view(x.size(0), -1)  # 调整输入尺寸为(batch_size, 28*28)\n",
    "        \n",
    "        h1 = F.relu(self.mul_h1(x))\n",
    "        h2 = self.mul_h2(h1)\n",
    "        \n",
    "        return h2\n",
    "        \n",
    "model = myModel()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    计算交叉熵损失。\n",
    "    :param logits: 模型输出的未归一化logits\n",
    "    :param labels: 真实标签（类别索引）\n",
    "    :return: 标量损失\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return criterion(logits, labels)\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    \"\"\"\n",
    "    计算准确率。\n",
    "    :param logits: 模型输出的未归一化logits\n",
    "    :param labels: 真实标签（类别索引）\n",
    "    :return: 准确率\n",
    "    \"\"\"\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    return torch.mean((predictions == labels).float())\n",
    "\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    \"\"\"\n",
    "    执行一个训练步骤。\n",
    "    :param model: 模型实例\n",
    "    :param optimizer: 优化器实例\n",
    "    :param x: 输入数据\n",
    "    :param y: 真实标签\n",
    "    :return: 损失和准确率\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    \n",
    "    logits = model(x)\n",
    "    loss = compute_loss(logits, y)\n",
    "    \n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "    \n",
    "    accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "def test(model, x, y):\n",
    "    \"\"\"\n",
    "    测试模型性能。\n",
    "    :param model: 模型实例\n",
    "    :param x: 输入数据\n",
    "    :param y: 真实标签\n",
    "    :return: 测试集上的损失和准确率\n",
    "    \"\"\"\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(logits, y)\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "        \n",
    "    return loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.2748; accuracy 0.9215\n",
      "Epoch 1: loss 0.1237; accuracy 0.9628\n",
      "Epoch 2: loss 0.0858; accuracy 0.9737\n",
      "Epoch 3: loss 0.0662; accuracy 0.9790\n",
      "Epoch 4: loss 0.0530; accuracy 0.9833\n",
      "Epoch 5: loss 0.0437; accuracy 0.9855\n",
      "Epoch 6: loss 0.0363; accuracy 0.9883\n",
      "Epoch 7: loss 0.0295; accuracy 0.9901\n",
      "Epoch 8: loss 0.0267; accuracy 0.9911\n",
      "Epoch 9: loss 0.0210; accuracy 0.9930\n",
      "Epoch 10: loss 0.0203; accuracy 0.9929\n",
      "Epoch 11: loss 0.0170; accuracy 0.9944\n",
      "Epoch 12: loss 0.0161; accuracy 0.9943\n",
      "Epoch 13: loss 0.0155; accuracy 0.9946\n",
      "Epoch 14: loss 0.0148; accuracy 0.9949\n",
      "Epoch 15: loss 0.0117; accuracy 0.9959\n",
      "Epoch 16: loss 0.0118; accuracy 0.9959\n",
      "Epoch 17: loss 0.0111; accuracy 0.9961\n",
      "Epoch 18: loss 0.0099; accuracy 0.9966\n",
      "Epoch 19: loss 0.0121; accuracy 0.9957\n",
      "Epoch 20: loss 0.0085; accuracy 0.9970\n",
      "Epoch 21: loss 0.0089; accuracy 0.9968\n",
      "Epoch 22: loss 0.0074; accuracy 0.9974\n",
      "Epoch 23: loss 0.0088; accuracy 0.9969\n",
      "Epoch 24: loss 0.0109; accuracy 0.9966\n",
      "Epoch 25: loss 0.0095; accuracy 0.9966\n",
      "Epoch 26: loss 0.0071; accuracy 0.9975\n",
      "Epoch 27: loss 0.0060; accuracy 0.9981\n",
      "Epoch 28: loss 0.0089; accuracy 0.9969\n",
      "Epoch 29: loss 0.0073; accuracy 0.9976\n",
      "Epoch 30: loss 0.0082; accuracy 0.9975\n",
      "Epoch 31: loss 0.0067; accuracy 0.9976\n",
      "Epoch 32: loss 0.0073; accuracy 0.9975\n",
      "Epoch 33: loss 0.0050; accuracy 0.9984\n",
      "Epoch 34: loss 0.0066; accuracy 0.9978\n",
      "Epoch 35: loss 0.0095; accuracy 0.9969\n",
      "Epoch 36: loss 0.0069; accuracy 0.9977\n",
      "Epoch 37: loss 0.0066; accuracy 0.9977\n",
      "Epoch 38: loss 0.0046; accuracy 0.9984\n",
      "Epoch 39: loss 0.0066; accuracy 0.9978\n",
      "Epoch 40: loss 0.0078; accuracy 0.9975\n",
      "Epoch 41: loss 0.0045; accuracy 0.9985\n",
      "Epoch 42: loss 0.0051; accuracy 0.9985\n",
      "Epoch 43: loss 0.0098; accuracy 0.9971\n",
      "Epoch 44: loss 0.0054; accuracy 0.9982\n",
      "Epoch 45: loss 0.0062; accuracy 0.9980\n",
      "Epoch 46: loss 0.0050; accuracy 0.9982\n",
      "Epoch 47: loss 0.0062; accuracy 0.9978\n",
      "Epoch 48: loss 0.0043; accuracy 0.9984\n",
      "Epoch 49: loss 0.0056; accuracy 0.9981\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 测试模型\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; Test accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: test() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = mnist_dataset()\n",
    "# 训练循环\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "        loss, accuracy = train_one_step(model, optimizer, batch_x, batch_y)\n",
    "        epoch_loss += loss\n",
    "        epoch_accuracy += accuracy\n",
    "    \n",
    "    epoch_loss /= len(train_loader)\n",
    "    epoch_accuracy /= len(train_loader)\n",
    "    print(f'Epoch {epoch}: loss {epoch_loss:.4f}; accuracy {epoch_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss 0.1678; accuracy 0.9796\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "model.eval()\n",
    "eval_loss = 0.0\n",
    "eval_accuracy = 0.0\n",
    "for batch_x, batch_y in test_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
    "    loss, accuracy = test(model, batch_x, batch_y)\n",
    "    eval_loss += loss\n",
    "    eval_accuracy += accuracy\n",
    "\n",
    "eval_loss /= len(test_loader)\n",
    "eval_accuracy /= len(test_loader)\n",
    "print(f'Test: loss {eval_loss:.4f}; accuracy {eval_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n",
      "tensor([[-14.4693, -26.4977,  -7.8636,   5.2124, -53.7433,  -1.4998, -41.4715,\n",
      "         -22.1236,  27.8530, -33.1808]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(batch_y[7])\n",
    "print(model(batch_x[7]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
