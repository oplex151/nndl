## 解释RNN、LSTM、GRU模型

* RNN（循环神经网络）：RNN 的核心思想是利用序列信息，它通过在网络中引入循环来实现这一点。对于每个时间步的输入，RNN 不仅会考虑当前输入，还会考虑前一个时间步的输出（隐藏状态）。然而，传统的 RNN 在处理长距离依赖时遇到了挑战，比如梯度消失或爆炸问题。

  如下图，$y_1=act(vh_1+c),h1=f(ux_1+wh_0+b)$，以此类推

  <img src="%E6%8A%A5%E5%91%8A.assets/image-20250319132617842.png" alt="image-20250319132617842" style="zoom: 33%;" /><img src="%E6%8A%A5%E5%91%8A.assets/image-20250319132644149.png" alt="image-20250319132644149" style="zoom: 33%;" />    

* LSTM（长短期记忆网络）：为了解决传统 RNN 的局限性，LSTM 引入了一种称为“门控”的机制。LSTM 单元包括三个门：遗忘门（决定丢弃哪些信息）、记忆门（更新细胞状态）和输出门（决定下一层接收的信息）。这些门使用 sigmoid 函数和点乘操作来控制信息流，使得 LSTM 能够有效地学习长期依赖关系。

  如下图，上方的横线为长期记忆$c_{t-1}\to c_t$，下方为短期记忆$h_{t-1}\to h_t$

  遗忘门根据$h_(t-1)和x_t$决定长期记忆的保留率，$f_t=\sigma(W_f\cdot \left[h_{t-1},x_t\right]+b_f)$，用$f_t$去乘以$c_{t-1}$来实现遗忘

  记忆门根据$h_(t-1)和x_t$决定记住当前多少，当前信息$i_t=\sigma(W_i\cot[h_{t-1},x_t]+b_i)$，记忆率$C^{'}_t=tanh(W_c\cdot[h_{t-1},x_t]+b_c)$，两者相乘加到长期记忆上

  输出门负责给出当前神经元的输出，输出原始信息$o_t=\sigma(W_o\cot[h_{t-1},x_t]+b_o)$，输出率$tanh(c_t)=tanh(c_{t-1}\cdot f_t+i_t\cdot C^{'}_t)$，于是输出$h_t=o_t\cdot tanh(c_t)$

  ![image-20250319133036845](%E6%8A%A5%E5%91%8A.assets/image-20250319133036845.png)

* GRU（门控循环单元）：GRU 是 LSTM 的一种变体，旨在简化模型结构同时保留 LSTM 的关键特征。GRU 将 LSTM 中的遗忘门和输入门合并成一个单一的“更新门”，并结合了隐藏状态和细胞状态。这不仅减少了参数数量，还加速了训练过程。

  下图中

  $z_t=\sigma(W_z\cdot[h_{t-1},x_t])$

  $r_t=\sigma(W_r\cdot[h_{t-1},x_t])$

  $h^{'}_t=tanh(W_h\cdot[r_t* h_{t-1},x_t])$

  $h_t=(1-z_t)*h_{t-1}+z_t*h^{'}_t$

  <img src="%E6%8A%A5%E5%91%8A.assets/image-20250319134523721.png" alt="image-20250319134523721" style="zoom:50%;" />

## 叙述诗歌生成过程

1. **预处理**：首先需要对文本数据进行预处理，包括分词、建立词汇表以及将词语转换为索引。
2. **构建模型**：根据任务需求选择合适的模型架构LSTM或RNN，并定义相应的层和参数。
3. **训练模型**：使用大量文本数据来训练模型，使其能够学习到语言模式和语法规则。在训练过程中，模型尝试基于先前的词预测下一个词。
4. **生成诗歌**：
   - 从指定的起始词开始，将其转换为模型可以接受的形式——嵌入向量。
   - 使用训练好的模型预测下一个词的概率分布，并从中选取最可能的一个作为新的输入。
   - 重复此过程，直到生成满足条件的诗句长度或者遇到特定的终止符为止。
   - 最终，将生成的索引序列转换回单词序列以形成完整的诗歌

![image-20250319170042105](%E6%8A%A5%E5%91%8A.assets/image-20250319170042105.png)

## 生成诗歌

![image-20250324191006780](%E6%8A%A5%E5%91%8A.assets/image-20250324191006780.png)
